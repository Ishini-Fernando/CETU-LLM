{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80900401-d1c0-4a69-b65b-c53586076a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Paragraph  \\\n",
      "0  Patient of Patient ID 1 is a smoker and an alc...   \n",
      "1  Patient of Patient ID 1 is a smoker and an alc...   \n",
      "2  Patient of Patient ID 1 is a smoker and an alc...   \n",
      "3  Patient of Patient ID 1 is a smoker and an alc...   \n",
      "4  Patient of Patient ID 1 is a smoker and an alc...   \n",
      "\n",
      "                                           Sentences  \\\n",
      "0  [Patient of Patient ID 1 is a smoker and an al...   \n",
      "1  [Patient of Patient ID 1 is a smoker and an al...   \n",
      "2  [Patient of Patient ID 1 is a smoker and an al...   \n",
      "3  [Patient of Patient ID 1 is a smoker and an al...   \n",
      "4  [Patient of Patient ID 1 is a smoker and an al...   \n",
      "\n",
      "                                              Tokens  \n",
      "0  [[Patient, of, Patient, ID, 1, is, a, smoker, ...  \n",
      "1  [[Patient, of, Patient, ID, 1, is, a, smoker, ...  \n",
      "2  [[Patient, of, Patient, ID, 1, is, a, smoker, ...  \n",
      "3  [[Patient, of, Patient, ID, 1, is, a, smoker, ...  \n",
      "4  [[Patient, of, Patient, ID, 1, is, a, smoker, ...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Load the dataset\n",
    "dataset = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "# Preprocess the dataset\n",
    "def preprocess_text(text):\n",
    "    # Remove unwanted characters\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s.,']\", \"\", text)\n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "# Apply preprocessing to the dataset\n",
    "dataset['Paragraph'] = dataset['Paragraph'].apply(preprocess_text)\n",
    "\n",
    "# Split the paragraphs into sentences\n",
    "dataset['Sentences'] = dataset['Paragraph'].str.split('.')\n",
    "\n",
    "# Drop rows with empty sentences\n",
    "dataset = dataset.dropna()\n",
    "\n",
    "# Split the sentences into tokens\n",
    "dataset['Tokens'] = dataset['Sentences'].apply(lambda x: [sentence.split() for sentence in x])\n",
    "\n",
    "# Display the preprocessed dataset\n",
    "print(dataset.head())\n",
    "dataset.to_csv(\"preprocessed_dataset.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93c285e8-6e31-4b98-b2ce-665a372f52b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f869f6c0a12d430fbfac5246abaaf0d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ed7bff2ec6b470cadd9eac92d2d0368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7ca755a5bda419180def10bce7011d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61caffd28f0d48b090d2bfd9c8295f55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/357 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eaa34ded2d94ae6aa8de63de3a43068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af7d5ff631f9450e931d537eda336949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type gpt_neo to instantiate a model of type gpt2. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0548cefca60e41119d7ee847d256f3dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/526M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['transformer.h.0.attn.c_attn.bias', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.0.attn.c_proj.bias', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.1.attn.c_attn.bias', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.1.attn.c_proj.bias', 'transformer.h.1.attn.c_proj.weight', 'transformer.h.10.attn.c_attn.bias', 'transformer.h.10.attn.c_attn.weight', 'transformer.h.10.attn.c_proj.bias', 'transformer.h.10.attn.c_proj.weight', 'transformer.h.11.attn.c_attn.bias', 'transformer.h.11.attn.c_attn.weight', 'transformer.h.11.attn.c_proj.bias', 'transformer.h.11.attn.c_proj.weight', 'transformer.h.2.attn.c_attn.bias', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.2.attn.c_proj.bias', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.3.attn.c_attn.bias', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.3.attn.c_proj.bias', 'transformer.h.3.attn.c_proj.weight', 'transformer.h.4.attn.c_attn.bias', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.4.attn.c_proj.bias', 'transformer.h.4.attn.c_proj.weight', 'transformer.h.5.attn.c_attn.bias', 'transformer.h.5.attn.c_attn.weight', 'transformer.h.5.attn.c_proj.bias', 'transformer.h.5.attn.c_proj.weight', 'transformer.h.6.attn.c_attn.bias', 'transformer.h.6.attn.c_attn.weight', 'transformer.h.6.attn.c_proj.bias', 'transformer.h.6.attn.c_proj.weight', 'transformer.h.7.attn.c_attn.bias', 'transformer.h.7.attn.c_attn.weight', 'transformer.h.7.attn.c_proj.bias', 'transformer.h.7.attn.c_proj.weight', 'transformer.h.8.attn.c_attn.bias', 'transformer.h.8.attn.c_attn.weight', 'transformer.h.8.attn.c_proj.bias', 'transformer.h.8.attn.c_proj.weight', 'transformer.h.9.attn.c_attn.bias', 'transformer.h.9.attn.c_attn.weight', 'transformer.h.9.attn.c_proj.bias', 'transformer.h.9.attn.c_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized because the shapes did not match:\n",
      "- transformer.h.0.mlp.c_fc.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([768, 3072]) in the model instantiated\n",
      "- transformer.h.0.mlp.c_proj.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([3072, 768]) in the model instantiated\n",
      "- transformer.h.1.mlp.c_fc.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([768, 3072]) in the model instantiated\n",
      "- transformer.h.1.mlp.c_proj.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([3072, 768]) in the model instantiated\n",
      "- transformer.h.10.mlp.c_fc.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([768, 3072]) in the model instantiated\n",
      "- transformer.h.10.mlp.c_proj.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([3072, 768]) in the model instantiated\n",
      "- transformer.h.11.mlp.c_fc.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([768, 3072]) in the model instantiated\n",
      "- transformer.h.11.mlp.c_proj.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([3072, 768]) in the model instantiated\n",
      "- transformer.h.2.mlp.c_fc.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([768, 3072]) in the model instantiated\n",
      "- transformer.h.2.mlp.c_proj.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([3072, 768]) in the model instantiated\n",
      "- transformer.h.3.mlp.c_fc.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([768, 3072]) in the model instantiated\n",
      "- transformer.h.3.mlp.c_proj.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([3072, 768]) in the model instantiated\n",
      "- transformer.h.4.mlp.c_fc.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([768, 3072]) in the model instantiated\n",
      "- transformer.h.4.mlp.c_proj.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([3072, 768]) in the model instantiated\n",
      "- transformer.h.5.mlp.c_fc.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([768, 3072]) in the model instantiated\n",
      "- transformer.h.5.mlp.c_proj.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([3072, 768]) in the model instantiated\n",
      "- transformer.h.6.mlp.c_fc.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([768, 3072]) in the model instantiated\n",
      "- transformer.h.6.mlp.c_proj.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([3072, 768]) in the model instantiated\n",
      "- transformer.h.7.mlp.c_fc.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([768, 3072]) in the model instantiated\n",
      "- transformer.h.7.mlp.c_proj.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([3072, 768]) in the model instantiated\n",
      "- transformer.h.8.mlp.c_fc.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([768, 3072]) in the model instantiated\n",
      "- transformer.h.8.mlp.c_proj.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([3072, 768]) in the model instantiated\n",
      "- transformer.h.9.mlp.c_fc.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([768, 3072]) in the model instantiated\n",
      "- transformer.h.9.mlp.c_proj.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([3072, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "605c8d0a53874e4b993c486067b2f03d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19902' max='19902' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19902/19902 16:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.297600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.431000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.314800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.299400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.287200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.281200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.276400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.276900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.271200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.268400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.269200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.264200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.265400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.260100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.254800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.254900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.254000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.248400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.247500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.241900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.238900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.237300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.231200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.229200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.225900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.219100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.216600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.214500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.211400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.207900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.205400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.200500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.198600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.194800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.193500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.190900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.190400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.187300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.186400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=19902, training_loss=0.2937912958708855, metrics={'train_runtime': 1000.3378, 'train_samples_per_second': 79.578, 'train_steps_per_second': 19.895, 'total_flos': 5200038051840000.0, 'train_loss': 0.2937912958708855, 'epoch': 3.0})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "# Load GPT-Neo tokenizer and model with ignore_mismatched_sizes=True\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"EleutherAI/gpt-neo-125M\", ignore_mismatched_sizes=True)\n",
    "\n",
    "# Prepare the dataset for fine-tuning\n",
    "train_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"preprocessed_dataset.csv\",  # Path to your preprocessed dataset\n",
    "    block_size=128  # Set appropriate block size\n",
    ")\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./text-generation-model\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "# Define the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False,\n",
    "    ),\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bdc97a2f-795e-48be-bb27-2e77c0a63be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 is continuing treatment in the ETU.\",\"['Patient of Patient ID 9 is not a smoker and not an alcoholic female with Family History of Ischemic Heart Diseases at age 65 has 127 mmHg Systolic Blood Pressure, 80 mmHg Diastolic Blood Pressure, 80 beats per minute Heart Rate, 20 breaths per minute Respiratory Rate, 99 fahrenheit Body temperature, 100 mEqL SpO2, 139 mEqL Sodium Level, 4 mEqL Pot\n"
     ]
    }
   ],
   "source": [
    "def generate_next_hour(patient_info):\n",
    "    # Tokenize the input text\n",
    "    input_ids = tokenizer.encode(patient_info, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate text for the next hour\n",
    "    output = model.generate(input_ids, max_length=200, num_return_sequences=1, temperature=0.7)\n",
    "\n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract only the information for the next hour\n",
    "    next_hour_info = generated_text.split(\" at hour \")[-1]\n",
    "\n",
    "    return next_hour_info\n",
    "\n",
    "# Example usage\n",
    "patient_info = \"age 70 has 120 mmHg Systolic Blood Pressure, 70 mmHg Diastolic Blood Pressure, 70 beats per minute Heart Rate, 20 breaths per minute Respiratory Rate, 98 fahrenheit Body temperature, 98 mEqL SpO2\"\n",
    "next_hour_info = generate_next_hour(patient_info)\n",
    "print(next_hour_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c7f0f39-69d6-4b35-96be-0035b9c15ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define the directory path to save the model\n",
    "output_model_dir = \"./final-tuned-model\"\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained(output_model_dir)\n",
    "tokenizer.save_pretrained(output_model_dir)\n",
    "\n",
    "print(\"Model saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36baed73-b585-4229-aa65-1a34cc9e3cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is continuing treatment in the ETU. GENERATE ONLY THE Systolic Blood Pressure FOR THE NEXT HOUR astolic Blood Pressure, 75 beats per minute Heart Rate, 20 breaths per minute Respiratory Rate, 98 fahrenheit Body temperature, 98 mEqL SpO2, 139 mEqL Sodium Level, 4 mEqL\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the saved model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./final-tuned-model\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./final-tuned-model\")\n",
    "\n",
    "def generate_next_hour(patient_info):\n",
    "    # Tokenize the input text\n",
    "    input_ids = tokenizer.encode(patient_info, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate text for the next hour\n",
    "    output = model.generate(input_ids, max_length=200, num_return_sequences=1, temperature=0.7)\n",
    "\n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract only the information for the next hour\n",
    "    next_hour_info = generated_text.split(\" at hour \")[-1]\n",
    "\n",
    "    return next_hour_info\n",
    "\n",
    "# Example usage\n",
    "patient_info = \"Patient of Patient ID 10 is not a smoker and female with no Family History of Ischemic Heart Diseases at age 55 has 140 mmHg Systolic Blood Pressure, 70 mmHg Diastolic Blood Pressure, 80 beats per minute Heart Rate, 15 breaths per minute Respiratory Rate, 85 fahrenheit Body temperature , 98 mEq/L SpO2, 139 mEq/L Sodium Level, 4 mEq/L Potassium Level , 105 mEq/L Chloride Level, 41 mg/dL Urea, 91 mg/dL Creatinine at hour  is continuing treatment in the ETU. GENERATE ONLY THE Systolic Blood Pressure FOR THE NEXT HOUR \"\n",
    "next_hour_info = generate_next_hour(patient_info)\n",
    "print(next_hour_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c33a33b1-acaa-4678-b170-d033c4c4b0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 is continuing treatment in the ETU.\",\"['Patient of Patient ID 37 is a smoker and an alcoholic male with Family History of Ischemic Heart Diseases at age 51 has 95 mmHg Systolic Blood Pressure, 51 mmHg Diastolic Blood Pressure, 56 beats per minute Heart Rate, 18 breaths per minute Respiratory Rate, 98 fahrenheit Body temperature, 99 mEqL SpO2, 143 mEqL Sodium Level, 4 mEqL SpO2, 4 mEqL Sodium Level, 4 mEqL Potassium Level, 102 mEqL Chloride Level\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the saved model and tokenizer\n",
    "model_path = \"./final-tuned-model\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Set pad token ID\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "def generate_next_hour(patient_info, top_k=5):\n",
    "    # Tokenize the input text\n",
    "    input_ids = tokenizer.encode(patient_info, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate text for the next hour\n",
    "    output = model.generate(input_ids, \n",
    "                            max_length=250, \n",
    "                            num_return_sequences=1, \n",
    "                            temperature=0.7,\n",
    "                            pad_token_id=tokenizer.eos_token_id,\n",
    "                            top_k=top_k,\n",
    "                            top_p=0.95,\n",
    "                            do_sample=True,\n",
    "                            num_beams=1)\n",
    "\n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract only the information for the next hour\n",
    "    next_hour_info = generated_text.split(\" at hour \")[-1]\n",
    "\n",
    "    return next_hour_info\n",
    "\n",
    "# Example usage\n",
    "patient_info = \"Patient of Patient ID 50 is not a smoker and male\"\n",
    "next_hour_info = generate_next_hour(patient_info)\n",
    "print(next_hour_info)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
